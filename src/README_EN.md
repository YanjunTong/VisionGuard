```

VisionGurd

[ç®€ä½“ä¸­æ–‡](../README_CN.md) | English

A multimodal learning-based video anomaly detection system, specifically optimized for vehicle collision detection, supporting recognition and localization of various abnormal events.
![framework](./src/framework.png "framework")

Features

Â· ğŸ¯ Multimodal Fusion: Combines visual and textual information to improve detection accuracy
Â· â±ï¸ Temporal Localization: Precisely detects start and end times of abnormal events
Â· ğŸš€ Efficient Inference: Supports real-time video stream processing
Â· ğŸ”§ Weakly Supervised Learning: Trains temporal localization models with only video-level labels
Â· ğŸ“Š Multi-Task Learning: Simultaneously performs anomaly detection, event classification, and temporal localization

Supported Anomalous Events

Â· ğŸš— Vehicle Collision
Â· ğŸ”¥ Fire
Â· ğŸ‘Š Fighting
Â· ğŸ§ Falling
Â· âœ… Normal Scenes

Requirements

Â· Python 3.8+
Â· PyTorch 1.12+
Â· CUDA 11.0+ (GPU recommended)

Installation

1. Clone the repository:

```bash
git clone https://github.com/YanjunTong/VisionGuard.git
cd VisionGuard
```

1. Install dependencies:

```bash
pip install torch torchvision
pip install opencv-python pillow clip-by-openai
pip install numpy tqdm
```

Quick Start

Data Preprocessing

```bash
python process.py
```

Model Training

```bash
python train.py
```

Inference & Detection

```bash
python inference.py
```

Project Structure

```
video-anomaly-detection/
â”œâ”€â”€ process.py          # Data preprocessing and feature extraction
â”œâ”€â”€ train.py           # Model training script
â”œâ”€â”€ inference.py       # Inference and detection script
â”œâ”€â”€ preprocessed_data/ # Preprocessed feature storage
â”‚   â”œâ”€â”€ video_features/
â”‚   â”œâ”€â”€ text_features/
â”‚   â””â”€â”€ sim_matrices/
â”œâ”€â”€ saved_models/      # Trained model weights
â”œâ”€â”€ pseudo_labels/     # Pseudo-label data
â””â”€â”€ README.md
```

Data Preparation

Video Data

Place training videos in the train_videos/ directory and test videos in the video/ directory.

Text Descriptions

Configure video-text descriptions in process.py:

```python
TEST_TEXT_DESC_DICT = {
    "video_001": ["detect collision", "vehicle crash in video", ...],
    "normal_001": ["detect normal", "no abnormal events in video", ...]
}
```

Model Architecture

The system employs a three-head network structure:

Â· Fusion Module: CLIP features + Attention mechanism
Â· Anomaly Detection Head: Binary classification for anomaly detection
Â· Event Classification Head: Multi-class classification for event types
Â· Temporal Localization Head: Regression for event time offsets

Output Format

Inference results are saved in submission.txt with the format:

```
VideoID StartFrame EndFrame EventType
Example: car_01 125 189 Vehicle_Collision
```

Training Configuration

Key training parameters:

Â· Batch Size: 32
Â· Learning Rate: 1e-4
Â· Epochs: 500
Â· Frames per Clip: 16
Â· Sliding Stride: 8

Performance Optimization

Â· Uses CLIP ViT-B/32 model to balance accuracy and speed
Â· Sliding window strategy to avoid missed detections
Â· Feature pre-computation to accelerate training

License

MIT License

Citation

If you use this project, please cite:

```bibtex
@software{VideoAnomalyDetection2024,
  title = {VisionGuard},
  author = {Tong, Yanjun and Liang, Tianyv},
  year = {2025},
  url = {https://github.com/YanjunTong/VisionGuard}
}
```

Contributing

Issues and Pull Requests are welcome!

Contact

Â· Email: yanjun_tong@outlook.com
Â· GitHub: @yanjuntong

```